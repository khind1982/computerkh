#!/usr/bin/env python3
'''
usage: hmsloader [-h] -i {dev,pre,prod} [-b S3_BUCKET] [-v] [-n] [-d] [-f]
                 [-c COPYRIGHT_DIR] [-o OUTPUT_DIR] [-D DPMI_DIR]
                 [-C CLIENT_ID] [-S] [-T TEST_ID]
                 prefix root_dir

Submit objects to HMS for processing and storage.

positional arguments:
  prefix             The S3 prefix where items for the current load are to be
                     found
  root_dir           Directory to scan for objects to submit

optional arguments:
  -h, --help         show this help message and exit
  -i {dev,pre,prod}  Specify the HMS instance to target
  -b S3_BUCKET       S3 bucket to use as the staging post
  -v                 Be verbose
  -n                 If specified, do not attempt to load objects to S3 before
                     submitting to HMS
  -d                 Dry run. Don't send anything to S3, just save a copy of
                     the payload and headers to file
  -f                 Force reloading of objects to S3 before sending to HMS
  -c COPYRIGHT_DIR   Directory where the copyright text lookup files live
  -o OUTPUT_DIR      Directory where to write HMS responses
  -D DPMI_DIR        Directory where to find the DPMI files
  -C CLIENT_ID       Specify the HMS client ID. Almost always "CH"
  -S                 Stop execution after POST submitted to HMS. Only useful
                     for testing
  -T TEST_ID         Specify a fake book ID for testing purposes

hmsloader inspects the files located under <root_dir> and attempts to load them
to HMS using the ComplexObject recipe. Things are never quite that simple, though,
and it does a lot of work to prepare its payload.

First, it looks at the contents of <root_dir>, recording any subdirectory
ending in '-000' as containing cover/end/edge images. For other subdirectories,
it treats each as a ComplexObject; each subdirectory's contents will be submitted
as a single document. Before that happens, hmsloader walks down through the
directory, getting each file's size in bytes and calculating a hashed name (
using the first 5 characters of the filename's MD5 checksum prepended to the
filename), and then calulating a checksum from its contents. The checksum is
cached on disk to save time later.

Once the file metadata is collected, hmsloader will look for the objects in
S3, by sending a HEAD request to the staging bucket, pointing to <prefix>
followed by the file's hashed name. If the object is found, its S3 location is
stashed for later use in the HMS payload; otehrwise, we try to load the file
from local storage to the staging bucket, with the name <prefix>/<hashed-name>.

Once all the files in the current directory are either located in S3 or loaded,
hmsloader moves on to constructing the payload to submit to HMS. It includes
instructions on how to process the images, including quality and size, and
how to generate a PDF from the images. This pyaload is submitted, and we then
enter into a polling cycle until HMS either returns the results of the request,
or reports an error condition.
'''

# EEB data is partially stored on S3 at
#
# s3://content-digitisation-incoming-dev/eeb-all-images

##### TODO
# Cache file size and hashed names. Use the same cache files as the checksum?


import json
import multiprocessing  # Run multiple processes to upload to S3
import os
import sys
import time

from collections import defaultdict
from glob import glob

import requests

# pylint: disable = import-error
import hms
import hms.cli as cli
import hms.utils as utils

from hms import DryrunException
from hms import HMSAlreadyLoadedException
from hms import HMSError
from hms import HMSIncompleteRequest
from hms import NotFoundError
from hms.service import s3_head, s3_load, do_upload_or_locate_in_s3
from hms.payload import build_json

# pylint: enable = import-error


TMPDIR = utils.TMPDIR

# Somewhere to keep our S3 connection and related bits, if we need them.
os.environ['AWS_SHARED_CREDENTIALS_FILE'] = "/dc/dsol/aws/credentials"
os.environ['AWS_CONFIG_FILE'] = "/dc/dsol/aws/config"


if __name__ == '__main__':
    # pylint: disable = invalid-name
    num_cpus = multiprocessing.cpu_count()
    num_procs = 2  # int(num_cpus / 2 + 1)
    args = cli.parse_args()
    args.num_procs = num_procs

    # Define a simple debug function. If the user didn't ask for added
    # verbosity, this is just a nop.
    if args.verbose:
        def debug(text):
            print(text, end='', file=sys.stderr, flush=True)
    else:
        def debug(_text):
            pass
    # Attach the debug() function to the utils module so we can use it
    # everywhere. This replaces the default implementation in the utils
    # module before any calls are made to it.
    utils.debug = debug

    # We are splitting books before submitting them to HMS, so that each
    # HMS load request corresponds to a "title", so that single-title volumes
    # are sent in their entirety in a single request, but "bound-with" volumes
    # are split into their constituent "books", and each is sent as a single
    # request. In each case, the end/cover/edge images are sent along as well,
    # so that every title-level object has a set of covers.

    dry_run_ids = []  # Stash IDs from a dry run so we can print a useful
                      # message at the end of the run
    for directory in sorted(os.listdir(args.root_dir)):
        # Don't treat directories ending in '000' as book IDs.
        if directory.endswith('000'):
            args.cover_images_path = os.path.join(args.root_dir, directory)
            continue

        args.book_images_path = os.path.join(args.root_dir, directory)
        args.book_id = book_id = directory

        # If the user requested a dry run, the code that logs the request and
        # headers raises DryrunException. In order to allow us to move to the
        # next item in a bound-with book, we need to catch that exception here
        # and explicitly call continue.
        try:
            hms.handle_book(book_id, args)
        except DryrunException as exc:
            dry_run_ids.append(exc)
            continue
        except HMSAlreadyLoadedException:
            # If the book is already loaded, hms.lookup will raise
            # HMSAlreadyLoadedException. We need to catch it here so we
            # can continue through the loop in the event we're looking at
            # a bound-with volume.
            continue
        except HMSError as exc:
            print("HMS error: %s" % exc.message)
            print("Giving up on this book")
            continue

    if dry_run_ids:
        debug("This was a dry run. You can inspect the JSON and HTTP headers ")
        debug("for the requested book(s) in:\n")
        for book in dry_run_ids:
            debug(book)


# Local Variables:
# eval: (pyenv-use "3.6.1")
# End:
